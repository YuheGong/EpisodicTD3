FetchReacher-v:
  algorithm: promp_td3
  algo_params:
    learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:FetchReacher-v
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

### ALR Reacher ENV with different goal
ALRReacherBalance-v:
  algorithm: promp_td3
  algo_params:
    learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 2.e+6
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalance-v
    num_envs: 1

ALRReacherBalanceIP-v:
  algorithm: promp_td3
  algo_params:
    learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 2.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalanceIP-v
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

Ant-v0:
  algorithm: promp_td3
  algo_params:
    actor_learning_rate: 0.00001
    critic_learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    width: 0.01
    weight_scale: 10
    policy_type: velocity
    zero_start: False
  env_params:
    env_name: alr_envs:Ant-v0
    num_envs: 1