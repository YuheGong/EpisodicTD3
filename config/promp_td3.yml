### DeepMind Ball In Cup ENV with different reward function
DeepMindBallInCup-v0:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCup-v0
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCup-v1:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCup-v1
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCup-v2:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCup-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCupDense-v0:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 500
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCupDense-v0
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCupDense-v1:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 200
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCupDense-v1
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500

DeepMindBallInCupDense-v2:
  name: DeepMind_Ball_In_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    special_callback: DMbicCallback
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:DeepMindBallInCupDense-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500



### ALR Reacher ENV with different goal
ALRReacherBalance-v0:
  algorithm: sac
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 5.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalance-v0
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

ALRReacherBalance-v1:
  algorithm: promp_td3
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+7
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalance-v1
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

ALRReacherBalance-v2:
  algorithm: promp_td3
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+7
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalance-v2
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

ALRReacherBalanceSparse-v2:
  algorithm: promp_td3
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps:  2000
    total_timesteps: 5.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalanceSparse-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

GradientALRReacherBalanceSparseProMP-v0:
  name: ALRReacher + ProMP
  algorithm: promp_td3
  algo_params:
    x_init: 0
    dimension: 25
    sigma0: 0.1
    popsize: 10
    iteration: 5000
    stop_criterion: None
    batch_size: 200
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:f'ALRReacherBalanceSparseProMP-v2'
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000



### Meta World
MetaButtonPress-v2:
  algorithm: sac
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:MetaButtonPress-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

MetaButtonPressSparse-v2:
  algorithm: sac
  algo_params:
    batch_size: 200
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  env_params:
    env_name: alr_envs:MetaButtonPressSparse-v2
    num_envs: 1
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000




### other ENVs
ALRBallInACupSimpleDense-v0:
  name: Ball_In_A_Cup + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 5.e+6
    special_callback: ALRBallInACupCallback
    special_policy: CustomActorCriticPolicy
    policy_type: off_policy
  env_params:
    env_name: alr_envs:ALRBallInACupSimpleDense-v0
    num_envs: 8
    wrapper: VecNormalize


HoleReacherDense-v0:
  name: HoleReacherDense + SAC
  algorithm: sac
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: off_policy
  env_params:
    env_name: alr_envs:HoleReacherDense-v0
    num_envs: 8
    wrapper: VecNormalize
  eval_env:
    n_eval_episode: 10
    eval_freq: 500


DeepMindWalkerDense-v0:
  name: DeepMind_Walker + PPO
  algorithm: ppo
  algo_params:
    batch_size: 50
    learning_rate: 0.0001
    n_steps: 2000
    total_timesteps: 2.e+6
    policy: MlpPolicy
    policy_type: off_policy
  env_params:
    env_name: alr_envs:DeepMindWalkerDense-v0
    num_envs: 8
    wrapper: VecNormalize
