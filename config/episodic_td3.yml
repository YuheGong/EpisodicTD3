FetchReacher-v1:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.0001
    critic_learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    controller_type: position
    controller_kwargs:
      p_gains: 0.1
      d_gains: 0.01
    width: 0.025
    weight_scale: 1
    zero_start: True
    zero_basis: 2
  env_params:
    env_name: alr_envs:FetchReacher-v1
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

### ALR Reacher ENV with different goal
ALRReacherBalance-v:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.00001
    critic_learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalance-v
    num_envs: 1

ALRReacherBalanceIP-v3:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.00001
    critic_learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    controller_type: motor
    controller_kwargs:
      p_gains: 1
      d_gains: 0.1
    num_basis: 10
    zero_start: True
    zero_basis: 2
    width: 0.1
    weight_scale: 1
  env_params:
    env_name: alr_envs:ALRReacherBalanceIP-v3
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

Ant-v0:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.000001
    critic_learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    policy_kwargs:
      p_gains: 1
      d_gains: 0.1
    width: 0.025
    weight_scale: 1
    policy_type: motor
    zero_start: True
  env_params:
    env_name: alr_envs:Ant-v0
    num_envs: 1

Ant-v1:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.000001
    critic_learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+7
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    controller_type: velocity
    controller_kwargs:
      p_gains: 0.1
      d_gains: 0.01
    width: 0.025
    weight_scale: 1
    zero_start: False
    zero_basis: 0
  env_params:
    env_name: alr_envs:Ant-v1
    num_envs: 1


Hopper-v0:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.00001
    critic_learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+7
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    zero_start: False
    policy_kwargs:
      p_gains: 0.1
      d_gains: 0.000001
    width: 0.01
    weight_scale: 1
    policy_type: motor
  env_params:
    env_name: alr_envs:Hopper-v0
    num_envs: 1

DeepMindWalkerDense-v0:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.00001
    critic_learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+7
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    zero_start: True
    zero_basis: 2
    controller_type: motor
    controller_kwargs:
      p_gains: 0.05
      d_gains: 0.005
    width: 0.01
    weight_scale: 1
  env_params:
    env_name: alr_envs:DeepMindWalkerDense-v0
    num_envs: 1

dmcCheetahDense-v0:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.01
    critic_learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+7
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    num_basis: 10
    zero_start: True
    zero_basis: 2
    controller_type: velocity
    controller_kwargs:
      p_gains: 10
      d_gains: 0.001
    width: 1
    weight_scale: 1
  env_params:
    env_name: alr_envs:dmcCheetahDense-v0
    num_envs: 1



MetaButtonPress-v2:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.0001
    critic_learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    num_basis: 10
    zero_start: False
    zero_basis: 0
    controller_type: position
    controller_kwargs:
      p_gains: 10
      d_gains: 0.001
    width: 0.025
    weight_scale: 1
  env_params:
    env_name: alr_envs:MetaButtonPress-v2
    num_envs: 1


InvertedDoublePendulum-v0:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.00001
    critic_learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    num_basis: 10
    zero_start: True
    zero_basis: 2
    controller_type: motor
    controller_kwargs:
      p_gains: 1
      d_gains: 0.0001
    width: 0.1
    weight_scale: 1
  env_params:
    env_name: alr_envs:InvertedDoublePendulum-v0
    num_envs: 1