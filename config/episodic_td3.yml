FetchReacher-v1:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.0001
    critic_learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    controller_type: position
    controller_kwargs:
      p_gains: 0.1
      d_gains: 0.01
    width: 0.025
    weight_scale: 1
    zero_start: True
    zero_basis: 2
  env_params:
    env_name: alr_envs:FetchReacher-v1
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

### ALR Reacher ENV with different goal
ALRReacherBalance-v:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.00001
    critic_learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
      qf: 256
  env_params:
    env_name: alr_envs:ALRReacherBalance-v
    num_envs: 1

ALRReacherBalanceIP-v3:
  algorithm: episodic_td3
  algo_params:
    initial_promp_params: 1
    actor_learning_rate: 0.00001
    critic_learning_rate: 0.0001
    critic_initial_seed: 1
    noise_sigma: 0.3
    n_steps: 20000
    total_timesteps: 1.e+6
    train_freq: 1
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    controller_type: motor
    controller_kwargs:
      p_gains: 1
      d_gains: 0.1
    num_basis: 10
    zero_start: True
    zero_basis: 2
    width: 0.1
    weight_scale: 1
  env_params:
    env_name: alr_envs:ALRReacherBalanceIP-v3
    num_envs: 1
    wrapper: None
  eval_env:
    n_eval_episode: 10
    eval_freq: 1000

Ant-v0:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.000001
    critic_learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    policy_kwargs:
      p_gains: 1
      d_gains: 0.1
    width: 0.025
    weight_scale: 1
    policy_type: motor
    zero_start: True
  env_params:
    env_name: alr_envs:Ant-v0
    num_envs: 1

Ant-v1:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.000001
    critic_learning_rate: 0.0001
    n_steps:  20000
    total_timesteps: 1.e+7
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    controller_type: velocity
    controller_kwargs:
      p_gains: 0.1
      d_gains: 0.01
    width: 0.025
    weight_scale: 1
    zero_start: False
    zero_basis: 0
  env_params:
    env_name: alr_envs:Ant-v1
    num_envs: 1


Hopper-v0:
  algorithm: episodic_td3
  algo_params:
    initial_promp_params: 0.1
    actor_learning_rate: 0.000005
    critic_learning_rate: 0.0001
    noise_sigma: 0.1
    n_steps:  20000
    total_timesteps: 1.e+7
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    num_basis: 20
    zero_start: False
    zero_basis: 0
    controller_type: velocity
    controller_kwargs:
      p_gains:
      d_gains:
    width: 0.01
    weight_scale: 0.2
  env_params:
    env_name: alr_envs:Hopper-v0
    num_envs: 1


dmcWalkerDense-v0:
  algorithm: episodic_td3
  algo_params:
    initial_promp_params:  -0.2
    actor_learning_rate: 0.001
    critic_learning_rate: 0.0001
    noise_sigma: 0.3
    total_timesteps: 2.e+6
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    num_basis: 20
    zero_start: False
    zero_basis: 0
    controller_type: velocity
    controller_kwargs:
      p_gains: 1
      d_gains: 0.01
    width: 0.01
    weight_scale: 1
  env_params:
    env_name: alr_envs:dmcWalkerDense-v0
    num_envs: 1

dmcCheetahDense-v0:
  algorithm: episodic_td3
  algo_params:
    initial_promp_params: 1
    actor_learning_rate:  0.0001
    critic_learning_rate: 0.0001
    critic_initial_seed: 2
    total_timesteps: 2.e+6
    policy: MlpPolicy
    policy_type: off_policy
    noise_sigma: 0.3
    policy_kwargs:
      activation_fn: tanh
      pi: 256
      qf: 256
  promp_params:
    num_basis: 20
    zero_start: False
    zero_basis: 2
    controller_type: motor
    controller_kwargs:
      p_gains: 0, 0, 0, 0, 0, 0
      d_gains: 0.1, 0.01, 0.05, 0.01, 0.01, 0.01
      i_gains:
    width: 0.01
    weight_scale: 1
  env_params:
    env_name: alr_envs:dmcCheetahDense-v0
    num_envs: 1


dmcHopperDense-v0:
  algorithm: episodic_td3
  algo_params:
    initial_promp_params: 1.e-7
    actor_learning_rate: 0.00001
    critic_learning_rate: 0.0001
    total_timesteps: 1.e+7
    policy: MlpPolicy
    policy_type: off_policy
    noise_sigma: 0.1
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    num_basis: 50
    zero_start: False
    zero_basis: 0
    controller_type: velocity
    controller_kwargs:
      p_gains: 1
      d_gains: 0.1
    width: 0.01
    weight_scale: 0.2
  env_params:
    env_name: alr_envs:dmcHopperDense-v0
    num_envs: 1

dmcSwimmerDense-v0:
  algorithm: episodic_td3
  algo_params:
    noise_sigma: 0.1
    initial_promp_params: 1
    actor_learning_rate: 0.0001
    critic_learning_rate: 0.0001
    total_timesteps: 1.e+7
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    num_basis: 10
    zero_start: False
    zero_basis: 0
    controller_type: velocity
    controller_kwargs:
      p_gains: 10
      d_gains: 10
    width: 0.01
    weight_scale: 1
  env_params:
    env_name: alr_envs:dmcSwimmerDense-v0
    num_envs: 1



MetaButtonPress-v2:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.0001
    critic_learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    num_basis: 10
    zero_start: False
    zero_basis: 0
    controller_type: position
    controller_kwargs:
      p_gains: 10
      d_gains: 10
    width: 2
    weight_scale: 50
  env_params:
    env_name: alr_envs:MetaButtonPress-v2
    num_envs: 1


InvertedDoublePendulum-v0:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.0001
    critic_learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    num_basis: 10
    zero_start: True
    zero_basis: 2
    controller_type: motor
    controller_kwargs:
      p_gains: 10
      d_gains: 0.1
      i_gains: 0
    width: 0.01
    weight_scale: 0.2
  env_params:
    env_name: alr_envs:InvertedDoublePendulum-v0
    num_envs: 1


ALRHalfCheetahJump-v0:
  algorithm: episodic_td3
  algo_params:
    actor_learning_rate: 0.001
    critic_learning_rate: 0.0001
    n_steps: 20000
    total_timesteps: 1.e+6
    policy: MlpPolicy
    policy_type: off_policy
    policy_kwargs:
      activation_fn: ReLU
      pi: 256
      qf: 256
  promp_params:
    num_basis: 10
    zero_start: True
    zero_basis: 2
    controller_type: pid
    controller_kwargs:
      p_gains: 1
      d_gains: 0.1
      i_gains: 0.01
    width: 0.01
    weight_scale: 1
  env_params:
    env_name: alr_envs:ALRHalfCheetahJump-v0
    num_envs: 1